
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />
<meta content="Interview resource of Data Science Interview focusing on Regression." lang="en" name="description" xml:lang="en" />
<meta content="interview, data science, machine learning, Regression, Linear Regression" name="keywords" />
<meta content="en_US" property="og:locale" />

    <title>Regression &#8212; The Data Science Interview Book</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/myfile.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="canonical" href="https://dipranjan.github.io/dsinterviewqns/contents/Algorithms/Regression.html" />
    <link rel="shortcut icon" href="../../_static/logo.gif"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Generative vs Discriminative Models" href="Generative%20VS%20Discriminative%20Models.html" />
    <link rel="prev" title="Hyperparameter Optimization" href="../Data/Hyperparameter%20optimization.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-60403888-2', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint">✨Book Last updated on 8 Oct'22 ✨ PDF version of the book is now available 🥳 Check the footer for the link!!!</div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.gif" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">The Data Science Interview Book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    About this Book
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Journey of this project
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../To%20Do%20List.html">
   Log
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Statistics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Statistics/Probability%20Basics.html">
   Probability Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Statistics/Probability%20Distribution.html">
   Probability Distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Statistics/Central%20Limit%20Theorem.html">
   Central Limit Theorem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Statistics/Bayesian%20vs%20Frequentist%20Reasoning.html">
   Bayesian vs Frequentist Reasoning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Statistics/Hypothesis%20Testing.html">
   Hypothesis Testing
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Model Building
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Data/Data.html">
   Data
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Data/Scaling.html">
     Scaling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Data/Missing%20Value.html">
     Missing Value
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Data/Outlier.html">
     Outlier
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Data/Sampling.html">
     Sampling
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Data/Hyperparameter%20optimization.html">
   Hyperparameter Optimization
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Algorithms
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Generative%20VS%20Discriminative%20Models.html">
   Generative vs Discriminative Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Classification.html">
   Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Tree%20based%20approaches.html">
   Tree based approaches
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Time%20Series%20Analysis.html">
   Time Series Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Anomaly%20Detection.html">
   Anomaly Detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Big%20O%20Analysis.html">
   Big O Analysis
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Neural Network
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../NN/Neural%20Network.html">
   Neural Network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../NN/Recurrent%20Neural%20Network.html">
   Recurrent Neural Network
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  NLP
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../NLP/Lexical%20Processing.html">
   Lexical Processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../NLP/Syntactic%20Processing.html">
   Syntactic Processing
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Story Telling
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../StoryTelling/Visualization.html">
   Visualization
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Python
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Python/Basics.html">
   Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Python/Data%20Manipulation.html">
   Data Manipulation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Python/Statistics.html">
   Statistics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Python/NLP.html">
   NLP
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  SQL
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../SQL/Select.html">
   SQL Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../SQL/Joins.html">
   Joins
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../SQL/Temporary%20Datasets.html">
   Temporary Datasets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../SQL/Windows%20Functions.html">
   Windows Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../SQL/Time.html">
   Time
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../SQL/Functions.html">
   Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../SQL/Problems.html">
   Problems
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Excel
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Excel/Excel%20Basics.html">
   Excel Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Excel/Data%20Manipulation.html">
   Data Manipulation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Excel/Time%20and%20Date.html">
   Time and Date
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Machine Learning Frameworks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../MLFramework/PyCaret.html">
   PyCaret
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../MLFramework/Tensorflow/Tensorflow.html">
   Tensorflow
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../MLFramework/Tensorflow/Basics.html">
     Basics
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Analytical Thinking
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Analytical%20Thinking/Business%20Scenarios.html">
   Business Scenarios
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Analytical%20Thinking/Industry%20Application.html">
   Industry Application
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Analytical%20Thinking/Behavioral%20-%20Management.html">
   Behavioral - Management
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            <div>
    <a href="https://www.buymeacoffee.com/dearc"><img src="https://img.buymeacoffee.com/button-api/?text=Buy me a coffee&emoji=&slug=dearc&button_colour=b20a0a&font_colour=ffffff&font_family=Bree&outline_colour=ffffff&coffee_colour=FFDD00" /></a>
</div>

            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/dipranjan/dsinterviewqns/issues/new?title=Issue%20on%20page%20%2Fcontents/Algorithms/Regression.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>

</a>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/contents/Algorithms/Regression.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-regression">
   Linear Regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#metrics">
     Metrics
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#feature-selection">
     Feature selection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#assumptions">
     Assumptions
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#non-linear-regression">
   Non-Linear Regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#polynomial-regression">
     Polynomial Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regression-splines">
     Regression Splines
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generalized-additive-models">
     Generalized additive models
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#questions">
   Questions
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Regression</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-regression">
   Linear Regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#metrics">
     Metrics
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#feature-selection">
     Feature selection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#assumptions">
     Assumptions
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#non-linear-regression">
   Non-Linear Regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#polynomial-regression">
     Polynomial Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regression-splines">
     Regression Splines
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generalized-additive-models">
     Generalized additive models
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#questions">
   Questions
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="regression">
<h1>Regression<a class="headerlink" href="#regression" title="Permalink to this headline">#</a></h1>
<section id="linear-regression">
<h2>Linear Regression<a class="headerlink" href="#linear-regression" title="Permalink to this headline">#</a></h2>
<figure class="align-default" id="image2">
<a class="reference internal image-reference" href="../../_images/image8.PNG"><img alt="../../_images/image8.PNG" src="../../_images/image8.PNG" style="width: 376.2px; height: 228.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10 </span><span class="caption-text"><a class="reference external" href="https://xkcd.com/605/">📖Source</a></span><a class="headerlink" href="#image2" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p><span class="math notranslate nohighlight">\(Y = b_0 + b_1 * x_1 + b_2 * x_2 + \epsilon\)</span></p>
<p>The idea is to find the line or plane which best fits the data. Collectively, <span class="math notranslate nohighlight">\(b_0, b_1, b_2\)</span> are called regression coefficients. <span class="math notranslate nohighlight">\(\epsilon\)</span> is the error term, the part of <span class="math notranslate nohighlight">\(Y\)</span> the regression model is unable to explain.</p>
<figure class="align-default" id="id1">
<img alt="../../_images/image2.PNG" src="../../_images/image2.PNG" />
<figcaption>
<p><span class="caption-number">Fig. 11 </span><span class="caption-text"><a class="reference external" href="https://www.shutterstock.com/image-illustration/annotated-diagram-explaining-components-graph-showing-1406041139">📖Source</a></span><a class="headerlink" href="#id1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<section id="metrics">
<h3>Metrics<a class="headerlink" href="#metrics" title="Permalink to this headline">#</a></h3>
<p>Now once you have the model fit next comes the metrics to measure how good the fit is, some of the common metrics are as follows:</p>
<ul class="simple">
<li><p><strong><span class="math notranslate nohighlight">\(RSS\)</span></strong> (Residual sum of squares) <span class="math notranslate nohighlight">\(= (Y_{actual} - Y_{predicted})^2\)</span>, it changes with scale change</p></li>
<li><p><strong><span class="math notranslate nohighlight">\(TSS\)</span></strong> (Total sum of squares) <span class="math notranslate nohighlight">\(= (Y_{actual} - Y_{avg})^2\)</span></p></li>
<li><p><strong><span class="math notranslate nohighlight">\(R^2\)</span></strong> <span class="math notranslate nohighlight">\(= 1-\frac{RSS}{TSS} \)</span>, more the better, increases with more coefficients</p></li>
<li><p><strong><span class="math notranslate nohighlight">\(RSE\)</span></strong> (Residual Standard Error) <span class="math notranslate nohighlight">\( = \sqrt{\frac{RSS}{d.o.f}}\)</span>, here <span class="math notranslate nohighlight">\(d.o.f = n-2\)</span></p></li>
</ul>
</section>
<section id="feature-selection">
<h3>Feature selection<a class="headerlink" href="#feature-selection" title="Permalink to this headline">#</a></h3>
<p><a class="reference external" href="https://towardsdatascience.com/log-book-practical-guide-to-linear-polynomial-regression-in-r-e0ed2e7f8031">📖Explanation</a></p>
<ul>
<li><p>Hypothesis testing and using p-values to understand if the feature is important or not</p></li>
<li><p>Using metrics like <span class="math notranslate nohighlight">\(\text{Adjusted} R^2\)</span>, <span class="math notranslate nohighlight">\(AIC\)</span>, <span class="math notranslate nohighlight">\(BIC\)</span>, etc. which takes into consideration the number of features used to build the model and penalizes accordingly</p></li>
<li><p>How do we find the model that minimizes a metric like <span class="math notranslate nohighlight">\(AIC\)</span>? One approach is to search through all possible models, called all <strong>subset regression</strong>. This is computationally expensive and is not feasible for problems with large data and many variables. An attractive alternative is to use <strong>stepwise regression</strong> about which we learned above, this successively adds and drops predictors to find a model that lowers <span class="math notranslate nohighlight">\(AIC\)</span>. Simpler yet are <strong>forward selection</strong> and <strong>backward selection</strong>. In forward selection, you start with no predictors and add them one-by-one, at each step adding the predictor that has the largest contribution to , stopping when the contribution is no longer statistically significant. In backward selection, or backward elimination, you start with the full model and take away predictors that are not statistically significant until you are left with a model in which all predictors are statistically significant.</p></li>
<li><p><strong>Penalized Regression</strong> or <strong>Regularization</strong>:</p>
<p><a class="reference external" href="https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/">📖Explanation</a></p>
<p>Penalized regression is similar in spirit to AIC. Instead of explicitly searching through a discrete set of models, the model-fitting equation incorporates a constraint that penalizes the model for too many variables (parameters). Rather than eliminating predictor variables entirely — as with stepwise, forward, and backward selection — penalized regression applies the penalty by reducing coefficients, in some cases to near zero. Common penalized regression methods are ridge regression and lasso regression.
Regularization is nothing but adding a penalty term to the objective function and control the model complexity using that penalty term. It can be used for many machine learning Algorithms. Both Ridge and Lasso regression uses <span class="math notranslate nohighlight">\(L2\)</span> and <span class="math notranslate nohighlight">\(L1\)</span> regularizations.</p>
<ul class="simple">
<li><p><strong>Ridge Regression (L2)</strong>: <span class="math notranslate nohighlight">\(\text{RSS} + \lambda (\sum_{j=1}^p b_j^2)\)</span>, the value of hyperparameter <span class="math notranslate nohighlight">\(\lambda\)</span> can be found using cross-validation</p></li>
<li><p><strong>LASSO Regression (L1)</strong>: <span class="math notranslate nohighlight">\(\text{RSS} + \lambda (\sum_{j=1}^p \lVert b_j\rVert)\)</span>, the value of hyperparameter <span class="math notranslate nohighlight">\(\lambda\)</span> can be found using cross-validation. It’s full form is Least Absolute Shrinkage and Selection Operator</p></li>
</ul>
</li>
</ul>
<figure class="align-default" id="image9">
<a class="reference internal image-reference" href="../../_images/image9.PNG"><img alt="../../_images/image9.PNG" src="../../_images/image9.PNG" style="width: 536.9px; height: 402.5px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 12 </span><span class="caption-text">LASSO vs Ridge, the red contours are that of RSS whereas the geometric shapes are that of Ridge and Lasso. <a class="reference external" href="https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b">(📖Source)</a></span><a class="headerlink" href="#image9" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Ridge brings the coefficients close to <span class="math notranslate nohighlight">\(0\)</span> but not exactly to <span class="math notranslate nohighlight">\(0\)</span> which results in the model retaining all the features. Lasso on the other hand brings the coefficients to <span class="math notranslate nohighlight">\(0\)</span> hence results in reduced features. Lasso shrinks the coefficients by same amount whereas Ridge shrinks them by same proportion.</p>
<p>Elastic Net is another useful techniques which combines both L1 and L2 regularization.</p>
</div>
</section>
<section id="assumptions">
<h3>Assumptions<a class="headerlink" href="#assumptions" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>The relationship between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> is <strong>linear</strong>. Because we are fitting a linear model, we assume that the relationship really is linear, and that the errors, or residuals, are simply random fluctuations around the true line.</p></li>
<li><p>The error terms are <strong>normally distributed</strong>. This can be checked with a Q-Q plot</p></li>
<li><p>Error terms are independent of each other. This can be checked with a ACF plot. This can be used while checking independence while using a time-series data</p></li>
<li><p>Error terms are <strong>homoscedastic</strong>, i.e. they have constant variance. Residulas Vs Fitted graph should be flat. This means that the variability in the response is changing as the predicted value increases. This is a problem, in part, because the observations with larger errors will have more pull or influence on the fitted model.</p></li>
<li><p>The independent variables are not multicollinear. <strong>Multicollinearity</strong> is when a variable can be explained as a combination of other variables. This can be checked by using <strong>VIF(Variance inflation factor)</strong> <span class="math notranslate nohighlight">\(= \frac{1}{1-R_i^2}\)</span>.</p>
<ul>
<li><p>A VIF score of <span class="math notranslate nohighlight">\(&gt;10\)</span> indicates there there is a problem</p></li>
<li><p>If a multicollinear variable is present the coefficients swing wildly thereby affecting the interpretability of the model. P-vales are not reliable. But it doesnot affect prediction or the goodness of fit statistics.</p></li>
<li><p>To deal with multicollinearity</p>
<ul>
<li><p>drop variables</p></li>
<li><p>create new features from existing ones</p></li>
<li><p>PCA/PLS</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>One very important point to remember is that Generalized Linear Regression is called so because <span class="math notranslate nohighlight">\(Y\)</span> is linear w.r.t its coefficients <span class="math notranslate nohighlight">\(b_0, b_1, b_2\)</span>, etc. it is irrespective of whether the features <span class="math notranslate nohighlight">\(x_1, x_2\)</span>, etc. are linear or not. Meaning <span class="math notranslate nohighlight">\(x_1\)</span> can actually be <span class="math notranslate nohighlight">\(x_1^2\)</span> and it won’t matter.</p>
</div>
</section>
</section>
<section id="non-linear-regression">
<h2>Non-Linear Regression<a class="headerlink" href="#non-linear-regression" title="Permalink to this headline">#</a></h2>
<p>In some cases, the true relationship between the outcome and a predictor variable might not be linear.
There are different solutions extending the linear regression model for capturing these nonlinear effects, some of these are covered below.</p>
<section id="polynomial-regression">
<h3>Polynomial Regression<a class="headerlink" href="#polynomial-regression" title="Permalink to this headline">#</a></h3>
<p>The equation of polynomial becomes something like this.</p>
<p><span class="math notranslate nohighlight">\(Y = b_0 + b_1 * x_1 + b_2 * x_1^2 + b_n * x_1^n \)</span> and so on…</p>
<p>The degree of order which to use is a Hyperparameter, and we need to choose it wisely. But using a high degree of polynomial tries to overfit the data and for smaller values of degree, the model tries to underfit so we need to find the optimum value of a degree. <strong>Polynomial Regression on datasets with high variability chances to result in over-fitting.</strong></p>
</section>
<section id="regression-splines">
<h3>Regression Splines<a class="headerlink" href="#regression-splines" title="Permalink to this headline">#</a></h3>
<p><a class="reference external" href="https://www.analyticsvidhya.com/blog/2018/03/introduction-regression-splines-python-codes/">📖Explanation</a></p>
<p>In order to overcome the disadvantages of polynomial regression, we can use an improved regression technique which, instead of building one model for the entire dataset, divides the dataset into multiple bins and fits each bin with a separate model. Such a technique is known as Regression spline.</p>
<p>In polynomial regression, we generated new features by using various polynomial functions on the existing features which imposed a global structure on the dataset. To overcome this, we can divide the distribution of the data into separate portions and fit linear or low degree polynomial functions on each of these portions. The points where the division occurs are called <strong>Knots</strong>. Functions which we can use for modelling each piece/bin are known as Piecewise functions. There are various piecewise functions that we can use to fit these individual bins.</p>
</section>
<section id="generalized-additive-models">
<h3>Generalized additive models<a class="headerlink" href="#generalized-additive-models" title="Permalink to this headline">#</a></h3>
<p>It does the same thing as above but just removes the need to specifying the knots. It fits spline models with automated selection of knots.</p>
</section>
</section>
<section id="questions">
<h2>Questions<a class="headerlink" href="#questions" title="Permalink to this headline">#</a></h2>
<div class="tip dropdown admonition">
<p class="admonition-title">Problem: [UPSTART] Regression Coefficient</p>
<p>Suppose we have two variables, <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, where <span class="math notranslate nohighlight">\(Y = X +\)</span> some normal white noise.</p>
<ol class="simple">
<li><p>What will our coefficient be of we run a regression of <span class="math notranslate nohighlight">\(Y\)</span> on <span class="math notranslate nohighlight">\(X\)</span>?</p></li>
<li><p>What happens if we run a regression of <span class="math notranslate nohighlight">\(X\)</span> on <span class="math notranslate nohighlight">\(Y\)</span>?</p></li>
</ol>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Solution:</p>
<p>Let’s start with <span class="math notranslate nohighlight">\(Y = X\)</span>, then the regression line is a perfect fit. The points of such a dataset is <span class="math notranslate nohighlight">\((1,1),(2,2),(3,3),(4,4),(5,5)\)</span></p>
<p>Adding some normal white noise to these points <span class="math notranslate nohighlight">\((1,1),(2,3),(3,5),(4,5),(5,5)\)</span>. A regression line fit on these points will move up. Hence the coefficients of <span class="math notranslate nohighlight">\(Y = mX+c\)</span>, <span class="math notranslate nohighlight">\(m\)</span> will increase, <span class="math notranslate nohighlight">\(c\)</span> might still stay at <span class="math notranslate nohighlight">\(0\)</span> or at max increase.</p>
<p>This movement will go in the negative direction if we predict <span class="math notranslate nohighlight">\(X\)</span> based on <span class="math notranslate nohighlight">\(Y\)</span></p>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Problem: Linear Regression in Time Series</p>
<p>Do you think Linear Regression should be used in Time series analysis?</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Solution:</p>
<p>Linear Regression as per me can be used in Time Series but might not always give good results. Few reasons which come up are:</p>
<ul class="simple">
<li><p>Linear Regression is good for intrapolation but not for extrapolation so the results can vary wildly</p></li>
<li><p>When Linear Regression is used but observations are correlated (as in time series data) you will have a biased estimate of the variance</p></li>
<li><p>Moreover, time-series data have a pattern, such as during peak hours, festive seasons, etc., which would most likely be treated as outliers in the linear regression analysis</p></li>
</ul>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Problem: [AIRBNB] Booking Regression</p>
<p>Let’s say we want to build a model to predict booking prices.</p>
<ol class="simple">
<li><p>Explain the difference between a linear regression versus a random forest regression.</p></li>
<li><p>Which one would likely perform better?</p></li>
</ol>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Solution:</p>
<p>Linear Regression is used to predict continuous outputs where there is a linear relationship between the features of the dataset and the output variable. It is used for regression problems where you are trying to predict something with infinite possible answers such as the price of a house.</p>
<p>In the case of regression, decision trees in random forest learn by splitting the training examples in a way such that the sum of squared residuals is minimized. To classify a new object based on attributes, each tree gives a classification and we say the tree “votes” for that class. The forest chooses the classification having the most votes (over all the trees in the forest) and in case of regression, it takes the average of outputs by different trees. It is useful when there are complex relationships between the features and the output variables. They also work well compared to other Algorithms when there are missing features, when there is a mix of categorical and numerical features and when there is a big difference in the scale of features.</p>
<p>It is difficult to tell which will perform better, it completely depends on the problem statement and the available data. Other than the points mentioned above some of the Key advantages of linear models over tree-based ones are:</p>
<ul class="simple">
<li><p>they can extrapolate (e.g. if labels are between 1-5 in train set, tree based model will never predict 10, but linear will)</p></li>
<li><p>could be used for anomaly detection because of extrapolation</p></li>
<li><p>interpretability (yes, tree based models have feature importance, but it’s only a proxy, weights in linear model are better)</p></li>
<li><p>need less data to get good results</p></li>
<li><p>Random Forest is able to discover more complex relation at the cost of time</p></li>
</ul>
<p>The first point becomes clearly important in this case as we would need booking price values which might not necessarily be in the training data range.</p>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Problem: [GOOGLE] Adding Noise</p>
<p>Say we are running a probabilistic linear regression which does a good job modeling the underlying relationship between some <span class="math notranslate nohighlight">\(y\)</span> and <span class="math notranslate nohighlight">\(x\)</span>. Now assume all inputs have some noise <span class="math notranslate nohighlight">\(\epsilon\)</span> added, which is independent of the training data.</p>
<p>What is the new objective function? How do you compute it?</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Solution:</p>
<p><a class="reference external" href="https://www.nicksingh.com/posts/30-machine-learning-interview-questions-ml-interview-study-guide">📖Source</a></p>
<p>The objective function for linear regression where <span class="math notranslate nohighlight">\(x\)</span> is set of input vectors and <span class="math notranslate nohighlight">\(w\)</span> are the weights:
<span class="math notranslate nohighlight">\(L(w) = E[(w^Tx-y)^2]\)</span></p>
<p>Let’s assume that the noise added is Gaussian as follows: <span class="math notranslate nohighlight">\(\epsilon \sim N(0, \lambda I)\)</span>, then the new objective function is given by:<span class="math notranslate nohighlight">\(L(w) = E[(w^T(x + \epsilon)-y)^2]\)</span>.</p>
<p>To compute it, we simplify:
<span class="math notranslate nohighlight">\(L'(w) = E[(w^T x -y + w^T\epsilon)^2]\)</span>
<span class="math notranslate nohighlight">\(L'(w) = E[(w^T x - y)^2 + 2(w^Tx-y)w^T\epsilon +w^T\epsilon \epsilon^Tw]\)</span>
<span class="math notranslate nohighlight">\(L'(w) = E[(w^T x - y)^2] + E[2(w^Tx-y)w^T\epsilon] + E[w^T\epsilon \epsilon^Tw]\)</span></p>
<p>We know that the expectation for <span class="math notranslate nohighlight">\(\epsilon\)</span> is <span class="math notranslate nohighlight">\(0\)</span> so the middle term becomes <span class="math notranslate nohighlight">\(0\)</span> and we are left with:<span class="math notranslate nohighlight">\(L'(w) = L(w) + 0 + w^TE[\epsilon \epsilon^T]w\)</span></p>
<p>The last term can be simplified as: <span class="math notranslate nohighlight">\(L'(w) = L(w) + w^T\lambda Iw\)</span></p>
<p>And therefore the objective function simplifies to that of L2-regularization: <span class="math notranslate nohighlight">\(L'(w) = L(w) + \lambda||w||^2\)</span></p>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Problem: [UBER] L1 vs L2</p>
<p>What is L1 and L2 regularization? What are the differences between the two?</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Solution:</p>
<p><a class="reference external" href="https://www.nicksingh.com/posts/30-machine-learning-interview-questions-ml-interview-study-guide">📖Source</a></p>
<p><span class="math notranslate nohighlight">\(L1\)</span> and <span class="math notranslate nohighlight">\(L2\)</span> regularization are both methods of regularization that attempt to prevent overfitting in machine learning. For a regular regression model assume the loss function is given by <span class="math notranslate nohighlight">\(L\)</span>. <span class="math notranslate nohighlight">\(L1\)</span> adds the absolute value of the coefficients as a penalty term, whereas <span class="math notranslate nohighlight">\(L2\)</span> adds the squared magnitude of the coefficients as a penalty term.</p>
<p>The loss function for the two are:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Loss(L_1) = L + \lambda |w_i| \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(Loss(L_2) = L + \lambda |w_i^2| \)</span></p></li>
</ul>
<p>Where the loss function <span class="math notranslate nohighlight">\(L\)</span> is the sum of errors squared, given by the following, where <span class="math notranslate nohighlight">\(f(x)\)</span> is the model of interest, for example, linear regression with <span class="math notranslate nohighlight">\(p\)</span> predictors:</p>
<p><span class="math notranslate nohighlight">\(L = \sum_{i=1}^{n} (y_i - f(x_i))^2 = \sum_{i=1}^{n} (y_i - \sum_{j=1}^{p}(x_{ij}w_j) )^2 \space \text{for linear regression}\)</span></p>
<p>If we run gradient descent on the weights <span class="math notranslate nohighlight">\(w\)</span>, we find that <span class="math notranslate nohighlight">\(L1\)</span> regularization will force any weight closer to <span class="math notranslate nohighlight">\(0\)</span>, irrespective of its magnitude, whereas, for the <span class="math notranslate nohighlight">\(L2\)</span> regularization, the rate at which the weight goes towards <span class="math notranslate nohighlight">\(0\)</span> becomes slower as the rate goes towards <span class="math notranslate nohighlight">\(0\)</span>. Because of this, <span class="math notranslate nohighlight">\(L1\)</span> is more likely to “zero” out particular weights, and hence removing certain features from the model completely, leading to more sparse models.</p>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Problem: [TESLA] Choice of Cost Function</p>
<p>You’re working with several sensors that are designed to predict a particular energy consumption metric on a vehicle. Using the outputs of the sensors, you build a linear regression model to make the prediction. There are many sensors, and several of the sensors are prone to complete failure.</p>
<p>What are some cost functions you might consider, and which would you decide to minimize in this scenario?</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Solution:</p>
<p><a class="reference external" href="https://www.nicksingh.com/posts/30-machine-learning-interview-questions-ml-interview-study-guide">📖Source</a></p>
<p>There are two potential cost functions here, one using the <span class="math notranslate nohighlight">\(L1\)</span> norm and the other using the <span class="math notranslate nohighlight">\(L2\)</span> norm. Below are two basic cost functions using an L1 and L2 norm respectively:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(J(w) = ||Xw-y||\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(J(w) = |Xw-y|^2\)</span></p></li>
</ul>
<p>It would be more sensible to use an <span class="math notranslate nohighlight">\(L1\)</span> norm in this case since the <span class="math notranslate nohighlight">\(L1\)</span> norm penalizes the outliers harder and thus gives less weight to the complete failures than the <span class="math notranslate nohighlight">\(L2\)</span> norm does.</p>
<p>Additionally, it would be prudent to involve a regularization term to account for noise. If we assume that the noise added to each sensor uniformly as follows:
<span class="math notranslate nohighlight">\(\epsilon \sim N(0, \lambda I)\)</span> then using traditional <span class="math notranslate nohighlight">\(L2\)</span> regularization, we would have the cost function:
<span class="math notranslate nohighlight">\(J(w) = ||Xw-y|| + \lambda||w||^2\)</span></p>
<p>However, given the fact that there are many sensors (and a broad range of how useful they are), we could instead assume that noise is added by:
<span class="math notranslate nohighlight">\(\epsilon \sim N(0, \lambda D)\)</span>
where each diagonal term in the matrix D represents the error term used for each sensor (and hence penalizing certain sensors more than others). Then our final cost function is given by:
<span class="math notranslate nohighlight">\(J(w) = ||Xw-y|| + \lambda w^TDw\)</span></p>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Problem: [AIRBNB] Prove that maximizing the likelihood is equivalent to minimizing the sum of squared residuals</p>
<p>Suppose you are running a linear regression and model the error terms as being normally distributed. Show that in this setup, maximizing the likelihood of the data is equivalent to minimizing the sum of squared residuals.</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Solution:</p>
<p><a class="reference external" href="https://cppcodingzen.com/?p=1609">📖Source</a></p>
<p>A mathematical derivation like this requires us to:</p>
<ul class="simple">
<li><p>Define correct Mathematical symbols and their relationships through equations</p></li>
<li><p>Recall and use the definitions of the terms like <strong>likelihood</strong> and <strong>normally distributed</strong></p></li>
<li><p>Perform Mathematical manipulation to derive the required result</p></li>
</ul>
<p><strong>Problem Setup:</strong></p>
<p>A linear regression model proposes that the output <span class="math notranslate nohighlight">\(y\)</span> is linearly dependent on the input vector <span class="math notranslate nohighlight">\(X\)</span> by the relation,</p>
<p><span class="math notranslate nohighlight">\(y = W^T X + \beta\)</span></p>
<p>Where, <span class="math notranslate nohighlight">\(X\)</span> is an <span class="math notranslate nohighlight">\(m\)</span>-dimensional vector, and <span class="math notranslate nohighlight">\((W; \beta) = \{w_1, w_2, ..., w_m; \beta\}\)</span> are the parameters of the model.</p>
<p>Next, we are give a set of training data points, consisting of</p>
<ul class="simple">
<li><p>a set of input vectors, <span class="math notranslate nohighlight">\(X = \{X_1, X_2, ..., X_n\}\)</span>. Note that every input <span class="math notranslate nohighlight">\(X_i\)</span> is a vector, <span class="math notranslate nohighlight">\(X_i = \{X_{i1}, X_{i2}, ..., X_{im}\}\)</span></p></li>
<li><p>and a set of outputs <span class="math notranslate nohighlight">\(Y = \{y_1, y_2, ..., y_n\}\)</span></p></li>
</ul>
<p>Given the values of the parameters <span class="math notranslate nohighlight">\(W\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>, the estimate <span class="math notranslate nohighlight">\(\hat{y}\)</span> is given by</p>
<p><span class="math notranslate nohighlight">\(\hat{y}_i = \sum_{j=1}^m X_{ij} * w_j + \beta = X_{i1} * w_1 + X_{i2} * w_2 + ... + X_{im} * w_m + \beta \)</span>.</p>
<p>Finally, the error term for <span class="math notranslate nohighlight">\(i^{th}\)</span> input is simply the difference between the observed value <span class="math notranslate nohighlight">\(y\)</span> and the estimate <span class="math notranslate nohighlight">\(\hat{y}\)</span>.</p>
<p><span class="math notranslate nohighlight">\(\epsilon_i(W, \beta) = y_i - \hat{y}_i = y_i - \sum_{j=1}^m X_{ij} * w_j - \beta\)</span></p>
<p>Note that the error term depends on the parameters of the model <span class="math notranslate nohighlight">\(W\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>, and hence is denoted as <span class="math notranslate nohighlight">\(\epsilon_i(W, \beta)\)</span>.</p>
<p><strong>Likelihood:</strong></p>
<p>Take a look at the problem statement again. We are assuming that the error terms are normally distributed. There is an implicit assumption that all the error terms are independent of each other. (Make sure you make this assumption explicit to your interviewer).</p>
<p>What does it mean for the error term to be normally distributed. It means that, by definition, the probability distribution function of the <span class="math notranslate nohighlight">\(i^{th}\)</span> error term is given by</p>
<p><span class="math notranslate nohighlight">\(l(\epsilon_i|W; \beta) = \frac{1}{\sqrt{2\pi}\sigma}exp\{\frac{-\epsilon_i^2}{2\sigma^2}\}\)</span></p>
<p>This probability distribution function of the <span class="math notranslate nohighlight">\(i^{th}\)</span> input is also called its likelihood function, and also depends on the parameters of the model, <span class="math notranslate nohighlight">\(W\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>.</p>
<p>Since we are assuming that the error terms are also independent, their joint probability distribution, is given by the product of their likelihood.</p>
<p><span class="math notranslate nohighlight">\(l = \displaystyle \prod_{i=1}^n l(\epsilon_i|W; \beta)\)</span>
<span class="math notranslate nohighlight">\(    = \displaystyle \prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma}exp\{\frac{-\epsilon_i^2}{2\sigma^2}\}\)</span>
<span class="math notranslate nohighlight">\(  = \displaystyle \frac{1}{(\sqrt{2\pi}\sigma)^n} \prod_{i=1}^n exp\{\frac{-\epsilon_i^2}{2\sigma^2}\}\)</span>
<span class="math notranslate nohighlight">\( = \displaystyle \frac{1}{(\sqrt{2\pi}\sigma)^n} \prod_{i=1}^n exp\{\frac{-(y_i - \sum_{j=1}^m X_{ij} * w_j - \beta)^2}{2\sigma^2}\}\)</span></p>
<p><strong>Maximum Likelihood Estimator:</strong></p>
<p>The maximum likelihood estimator seeks to maximize the likelihood function defined above. For the maximization,</p>
<ul class="simple">
<li><p>We can ignore the constant <span class="math notranslate nohighlight">\(\frac{1}{(\sqrt{2\pi}\sigma)^n}\)</span></p></li>
<li><p>We can also take the log of the likelihood function, converting the product into sum</p></li>
</ul>
<p>The log likelihood function of the errors is given by</p>
<p><span class="math notranslate nohighlight">\(L = log(l)\)</span>
<span class="math notranslate nohighlight">\( = \displaystyle log(\prod_{i=1}^n exp\{\frac{-(y_i - \sum_{j=1}^m X_{ij} * w_j - \beta)^2}{2\sigma^2}\})\)</span>
<span class="math notranslate nohighlight">\( = \displaystyle \sum_{i=1}^n {\frac{-(y_i - \sum_{j=1}^m X_{ij} * w_j - \beta)^2}{2\sigma^2}}\)</span></p>
<p>As a final step, for the purpose of optimization, we can ignore the constant multiplier <span class="math notranslate nohighlight">\(2\sigma^2\)</span> from the summation, giving us</p>
<p><span class="math notranslate nohighlight">\(L = \displaystyle \sum_{i=1}^n -(y_i - \sum_{j=1}^m X_{ij} * w_j - \beta)^2\)</span></p>
<p>But this is just the <strong>negative of the sum of squared errors!</strong></p>
<p>Thus, if you want to maximize the likelihood (or log likelihood) of the errors, you better minimize the sum of squared errors of the estimates.</p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./contents\Algorithms"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../Data/Hyperparameter%20optimization.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Hyperparameter Optimization</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="Generative%20VS%20Discriminative%20Models.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Generative vs Discriminative Models</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Dip Ranjan Chatterjee | © MIT License - 2021<br/>
  
    <div class="extra_footer">
      <p>
<a href="https://dearc.medium.com/membership" target="_blank"><img src="https://img.shields.io/badge/Support-Medium%20Referral-lightgrey?style=flat-square&logo=appveyor.svg" alt="Medium Membership"></a>
<a href="https://www.linkedin.com/company/the-data-science-interview-book/?lipi=urn%3Ali%3Apage%3Ad_flagship3_feed%3BeglbXB3xT0mopZBzReqMEQ%3D%3D" target="_blank"><img src="https://img.shields.io/badge/Follow-LinkedIn-0077B5?style=flat-square&logo=appveyor.svg" alt="Follow LinkedIn"></a>
<a href="https://github.com/dipranjan/dsinterviewqns/discussions" target="_blank"><img src="https://img.shields.io/badge/Discussion%20Forum-%F0%9F%99%8A%20%20%F0%9F%99%88%20%20%F0%9F%99%89-0172B3?style=flat-square&logo=appveyor.svg" alt="Discussion Forum"></a>
<a href="https://www.buymeacoffee.com/dearc/e/88363" target="_blank"><img src="https://img.shields.io/badge/BUY-PDF%20Version%20%F0%9F%93%96-red?style=flat-square&logo=appveyor.svg" alt="Buy Book"></a>
</p>

    </div>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>