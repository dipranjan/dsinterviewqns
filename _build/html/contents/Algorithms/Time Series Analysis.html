
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />
<meta content="Interview resource of Data Science Interview focusing on Time Series Analysis." lang="en" name="description" xml:lang="en" />
<meta content="interview, data science, machine learning, Time Series, ARIMA, SARIMA, ARIMAX, Prophet" name="keywords" />
<meta content="en_US" property="og:locale" />

    <title>Time Series Analysis &#8212; The Data Science Interview Book</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/myfile.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="canonical" href="https://dipranjan.github.io/dsinterviewqns/contents/Algorithms/Time Series Analysis.html" />
    <link rel="shortcut icon" href="../../_static/logo.gif"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Anomaly Detection" href="Anomaly%20Detection.html" />
    <link rel="prev" title="Tree based approaches" href="Tree%20based%20approaches.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-60403888-2', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint">✨Book Last updated on 18 Oct'22 ✨ PDF version of the book is now available 🥳 Check the footer for the link!!!</div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.gif" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">The Data Science Interview Book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    About this Book
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Journey of this project
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../To%20Do%20List.html">
   Log
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Statistics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Statistics/Probability%20Basics.html">
   Probability Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Statistics/Probability%20Distribution.html">
   Probability Distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Statistics/Central%20Limit%20Theorem.html">
   Central Limit Theorem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Statistics/Bayesian%20vs%20Frequentist%20Reasoning.html">
   Bayesian vs Frequentist Reasoning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Statistics/Hypothesis%20Testing.html">
   Hypothesis Testing
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Model Building
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Data/Data.html">
   Data
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Data/Scaling.html">
     Scaling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Data/Missing%20Value.html">
     Missing Value
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Data/Outlier.html">
     Outlier
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Data/Sampling.html">
     Sampling
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Data/Hyperparameter%20optimization.html">
   Hyperparameter Optimization
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Algorithms
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Diagnostics.html">
   Bias/Variance Tradeoff
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Regression.html">
   Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Generative%20VS%20Discriminative%20Models.html">
   Generative vs Discriminative Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Classification.html">
   Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Tree%20based%20approaches.html">
   Tree based approaches
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Time Series Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Anomaly%20Detection.html">
   Anomaly Detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Big%20O%20Analysis.html">
   Big O Analysis
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Neural Network
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../NN/Neural%20Network.html">
   Neural Network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../NN/Recurrent%20Neural%20Network.html">
   Recurrent Neural Network
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  NLP
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../NLP/Lexical%20Processing.html">
   Lexical Processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../NLP/Syntactic%20Processing.html">
   Syntactic Processing
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Story Telling
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../StoryTelling/Visualization.html">
   Visualization
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Python
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Python/Basics.html">
   Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Python/Data%20Manipulation.html">
   Data Manipulation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Python/Statistics.html">
   Statistics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Python/NLP.html">
   NLP
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  SQL
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../SQL/Select.html">
   SQL Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../SQL/Joins.html">
   Joins
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../SQL/Temporary%20Datasets.html">
   Temporary Datasets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../SQL/Windows%20Functions.html">
   Windows Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../SQL/Time.html">
   Time
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../SQL/Functions.html">
   Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../SQL/Problems.html">
   Problems
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Excel
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Excel/Excel%20Basics.html">
   Excel Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Excel/Data%20Manipulation.html">
   Data Manipulation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Excel/Time%20and%20Date.html">
   Time and Date
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Machine Learning Frameworks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../MLFramework/PyCaret.html">
   PyCaret
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../MLFramework/Tensorflow/Tensorflow.html">
   Tensorflow
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../MLFramework/Tensorflow/Basics.html">
     Basics
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Analytical Thinking
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Analytical%20Thinking/Business%20Scenarios.html">
   Business Scenarios
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Analytical%20Thinking/Industry%20Application.html">
   Industry Application
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Analytical%20Thinking/Behavioral%20-%20Management.html">
   Behavioral - Management
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            <div>
    <a href="https://www.buymeacoffee.com/dearc"><img src="https://img.buymeacoffee.com/button-api/?text=Buy me a coffee&emoji=&slug=dearc&button_colour=b20a0a&font_colour=ffffff&font_family=Bree&outline_colour=ffffff&coffee_colour=FFDD00" /></a>
</div>

            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/dipranjan/dsinterviewqns/issues/new?title=Issue%20on%20page%20%2Fcontents/Algorithms/Time Series Analysis.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>

</a>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/contents/Algorithms/Time Series Analysis.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stationarity">
   Stationarity
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#arima-family">
   ARIMA family
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prophet">
   Prophet
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#metrics">
   Metrics
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#questions">
   Questions
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Time Series Analysis</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stationarity">
   Stationarity
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#arima-family">
   ARIMA family
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prophet">
   Prophet
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#metrics">
   Metrics
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#questions">
   Questions
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="time-series-analysis">
<h1>Time Series Analysis<a class="headerlink" href="#time-series-analysis" title="Permalink to this headline">#</a></h1>
<p><strong>Reference:</strong> <a class="reference external" href="https://nbviewer.jupyter.org/github/Yorko/mlcourse_open/blob/master/jupyter_english/topic09_time_series/topic9_part1_time_series_python.ipynb">📖Explanation</a></p>
<figure class="align-default" id="image14">
<a class="reference internal image-reference" href="../../_images/image14.PNG"><img alt="../../_images/image14.PNG" src="../../_images/image14.PNG" style="width: 361.9px; height: 349.29999999999995px;" /></a>
</figure>
<p>A time series is simply a series of data points ordered in time. In a time series, time is often the independent variable and the goal is usually to make a forecast for the future.</p>
<ul>
<li><p><strong>Moving Average:</strong> Here the assumption is that future value of our variable depends on the average of its <span class="math notranslate nohighlight">\(k\)</span> previous values. Moving average has another use case - smoothing the original time series to identify trends. The wider the window, the smoother the trend. In the case of very noisy data, which is often encountered in finance, this procedure can help detect common patterns. This can also be used to determine anamolies based on the confidence level.</p></li>
<li><p><strong>Weighted average:</strong> It is a simple modification to the moving average. The weights sum up to <code class="docutils literal notranslate"><span class="pre">1</span></code> with larger weights assigned to more recent observations.
<span class="math notranslate nohighlight">\(\hat{y}_{t} = \displaystyle\sum^{k}_{n=1} \omega_n y_{t+1-n}\)</span></p></li>
<li><p><strong>Exponential smoothing:</strong> Here instead of weighting the last <span class="math notranslate nohighlight">\(k\)</span> values of the time series, we start weighting all available observations while exponentially decreasing the weights as we move further back in time.
<span class="math notranslate nohighlight">\(\hat{y}_{t} = \alpha \cdot y_t + (1-\alpha) \cdot \hat y_{t-1} \)</span></p>
<p>The <span class="math notranslate nohighlight">\(\alpha\)</span> weight is called a smoothing factor. It defines how quickly we will “forget” the last available true observation. The smaller <span class="math notranslate nohighlight">\(\alpha\)</span> is, the more influence the previous observations have and the smoother the series is.</p>
</li>
<li><p><strong>Double Exponential smoothing:</strong> Up to now, the methods that we’ve discussed have been for a single future point prediction (with some nice smoothing). That is cool, but it is also not enough. Let’s extend exponential smoothing so that we can predict two future points (of course, we will also include more smoothing).</p>
<p>Series decomposition will help us – we obtain two components: intercept (i.e. level) <span class="math notranslate nohighlight">\(\ell\)</span> and slope (i.e. trend) <span class="math notranslate nohighlight">\(b\)</span>. We have learnt to predict intercept (or expected series value) with our previous methods; now, we will apply the same exponential smoothing to the trend by assuming that the future direction of the time series changes depends on the previous weighted changes. As a result, we get the following set of functions:</p>
<div class="math notranslate nohighlight">
\[\ell_x = \alpha y_x + (1-\alpha)(\ell_{x-1} + b_{x-1})\]</div>
<div class="math notranslate nohighlight">
\[b_x = \beta(\ell_x - \ell_{x-1}) + (1-\beta)b_{x-1}\]</div>
<div class="math notranslate nohighlight">
\[\hat{y}_{x+1} = \ell_x + b_x\]</div>
<p>The first one describes the intercept, which, as before, depends on the current value of the series. The second term is now split into previous values of the level and of the trend. The second function describes the trend, which depends on the level changes at the current step and on the previous value of the trend. In this case, the <span class="math notranslate nohighlight">\(\beta\)</span> coefficient is a weight for exponential smoothing. The final prediction is the sum of the model values of the intercept and trend.</p>
</li>
<li><p><strong>Triple exponential smoothing a.k.a. Holt-Winters:</strong></p>
<p>The idea is to add a third component - seasonality. This means that we should not use this method if our time series is not expected to have seasonality. Seasonal components in the model will explain repeated variations around intercept and trend, and it will be specified by the length of the season, in other words by the period after which the variations repeat. For each observation in the season, there is a separate component; for example, if the length of the season is 7 days (a weekly seasonality), we will have 7 seasonal components, one for each day of the week.</p>
<p>The new system of equations:</p>
<div class="math notranslate nohighlight">
\[\ell_x = \alpha(y_x - s_{x-L}) + (1-\alpha)(\ell_{x-1} + b_{x-1})\]</div>
<div class="math notranslate nohighlight">
\[b_x = \beta(\ell_x - \ell_{x-1}) + (1-\beta)b_{x-1}\]</div>
<div class="math notranslate nohighlight">
\[s_x = \gamma(y_x - \ell_x) + (1-\gamma)s_{x-L}\]</div>
<div class="math notranslate nohighlight">
\[\hat{y}_{x+m} = \ell_x + mb_x + s_{x-L+1+(m-1)modL}\]</div>
<p>The intercept now depends on the current value of the series minus any corresponding seasonal component. Trend remains unchanged, and the seasonal component depends on the current value of the series minus the intercept and on the previous value of the component. Take into account that the component is smoothed through all the available seasons; for example, if we have a Monday component, then it will only be averaged with other Mondays. You can read more on how averaging works and how the initial approximation of the trend and seasonal components is done <a class="reference external" href="http://www.itl.nist.gov/div898/handbook/pmc/section4/pmc435.htm">here</a>. Now that we have the seasonal component, we can predict not just one or two steps ahead but an arbitrary <span class="math notranslate nohighlight">\(m\)</span> future steps ahead, which is very encouraging.</p>
<p>Below is the code for a triple exponential smoothing model, which is also known by the last names of its creators, Charles Holt and his student Peter Winters. Additionally, the Brutlag method was included in the model to produce confidence intervals:</p>
<div class="math notranslate nohighlight">
\[\hat y_{max_x}=\ell_{x−1}+b_{x−1}+s_{x−T}+m⋅d_{t−T}\]</div>
<div class="math notranslate nohighlight">
\[\hat y_{min_x}=\ell_{x−1}+b_{x−1}+s_{x−T}-m⋅d_{t−T}\]</div>
<div class="math notranslate nohighlight">
\[d_t=\gamma∣y_t−\hat y_t∣+(1−\gamma)d_{t−T},\]</div>
<p>where <span class="math notranslate nohighlight">\(T\)</span> is the length of the season, <span class="math notranslate nohighlight">\(d\)</span> is the predicted deviation. Other parameters were taken from triple exponential smoothing. You can read more about the method and its applicability to anomaly detection in time series <a class="reference external" href="http://fedcsis.org/proceedings/2012/pliks/118.pdf">here</a>.</p>
<p>Exponentiality is hidden in the recursiveness of the function – we multiply by <span class="math notranslate nohighlight">\((1-\alpha)\)</span> each time, which already contains a multiplication by <span class="math notranslate nohighlight">\((1-\alpha)\)</span> of previous model values.</p>
</li>
</ul>
<section id="stationarity">
<h2>Stationarity<a class="headerlink" href="#stationarity" title="Permalink to this headline">#</a></h2>
<p>Before we start modeling, we should mention such an important property of time series, <strong>stationarity</strong>.</p>
<p>So why is stationarity so important? Because it is easy to make predictions on a stationary series since we can assume that the future statistical properties will not be different from those currently observed. Most of the time-series models, in one way or the other, try to predict those properties (mean or variance, for example). Furture predictions would be wrong if the original series were not stationary.</p>
<p>When running a linear regression the assumption is that all of the observations are all independent of each other. In a time series, however, we know that observations are time dependent. It turns out that a lot of nice results that hold for independent random variables (law of large numbers and central limit theorem to name a couple) hold for stationary random variables. So by making the data stationary, we can actually apply regression techniques to this time dependent variable.</p>
<p><strong>Dickey-Fuller test</strong> can be used as a check for stationarity. If ‘Test Statistic’ is greater than the ‘Critical Value’ then the time series is stationary.</p>
<p>There are a few ways to deal with non-stationarity:</p>
<ul class="simple">
<li><p>Deflation by CPI</p></li>
<li><p>Logarithmic</p></li>
<li><p>First Difference</p></li>
<li><p>Seasonal Difference</p></li>
<li><p>Seasonal Adjustment</p></li>
</ul>
<p>Plot the ACF and PACF charts and find the optimal parameters.</p>
</section>
<section id="arima-family">
<h2>ARIMA family<a class="headerlink" href="#arima-family" title="Permalink to this headline">#</a></h2>
<p>We will explain this model by building up letter by letter. <span class="math notranslate nohighlight">\(SARIMA(p, d, q)(P, D, Q, s)\)</span>, Seasonal Autoregression Moving Average model:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(AR(p)\)</span> - autoregression model i.e. regression of the time series onto itself. The basic assumption is that the current series values depend on its previous values with some lag (or several lags). The maximum lag in the model is referred to as <span class="math notranslate nohighlight">\(p\)</span>. To determine the initial <span class="math notranslate nohighlight">\(p\)</span>, you need to look at the PACF plot and find the biggest significant lag after which <strong>most</strong> other lags become insignificant.</p></li>
<li><p><span class="math notranslate nohighlight">\(MA(q)\)</span> - moving average model. Without going into too much detail, this models the error of the time series, again with the assumption that the current error depends on the previous with some lag, which is referred to as <span class="math notranslate nohighlight">\(q\)</span>. The initial value can be found on the ACF plot with the same logic as before.</p></li>
</ul>
<p>Let’s combine our first 4 letters:</p>
<p><span class="math notranslate nohighlight">\(AR(p) + MA(q) = ARMA(p, q)\)</span></p>
<p>What we have here is the Autoregressive–moving-average model! If the series is stationary, it can be approximated with these 4 letters. Let’s continue.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(I(d)\)</span> - order of integration. This is simply the number of nonseasonal differences needed to make the series stationary.</p></li>
</ul>
<p>Adding this letter to the four gives us the <span class="math notranslate nohighlight">\(ARIMA\)</span> model which can handle non-stationary data with the help of nonseasonal differences. Great, one more letter to go!</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(S(s)\)</span> - this is responsible for seasonality and equals the season period length of the series</p></li>
</ul>
<p>With this, we have three parameters: <span class="math notranslate nohighlight">\((P, D, Q)\)</span></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P\)</span> - order of autoregression for the seasonal component of the model, which can be derived from PACF. But you need to look at the number of significant lags, which are the multiples of the season period length. For example, if the period equals 24 and we see the 24-th and 48-th lags are significant in the PACF, that means the initial <span class="math notranslate nohighlight">\(P\)</span> should be 2.</p></li>
<li><p><span class="math notranslate nohighlight">\(Q\)</span> - similar logic using the ACF plot instead.</p></li>
<li><p><span class="math notranslate nohighlight">\(D\)</span> - order of seasonal integration. This can be equal to 1 or 0, depending on whether seasonal differeces were applied or not.</p></li>
</ul>
</section>
<section id="prophet">
<h2>Prophet<a class="headerlink" href="#prophet" title="Permalink to this headline">#</a></h2>
<p><a class="reference external" href="https://stats.stackexchange.com/questions/472266/inference-in-time-series-prophet-vs-arima">📖 Check out this discussion</a></p>
<p>ARIMA and similar models assume some sort of causal relationship between past values and past errors and future values of the time series. Facebook Prophet doesn’t look for any such causal relationships between past and future. Instead, it simply tries to find the best curve to fit to the data, using a linear or logistic curve, and Fourier coefficients for the seasonal components. There is also a regression component, but that is for external regressors, not for the time series itself (The Prophet model is a special case of GAM - Generalized Additive Model).</p>
</section>
<section id="metrics">
<h2>Metrics<a class="headerlink" href="#metrics" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><strong>R squared:</strong> coefficient of determination (in econometrics, this can be interpreted as the percentage of variance explained by the model), <span class="math notranslate nohighlight">\((-\infty, 1]\)</span></p></li>
</ul>
<p><span class="math notranslate nohighlight">\(R^2 = 1 - \frac{SS_{res}}{SS_{tot}}\)</span></p>
<ul class="simple">
<li><p><strong>Mean Absolute Error:</strong> this is an interpretable metric because it has the same unit of measurment as the initial series, <span class="math notranslate nohighlight">\([0, +\infty)\)</span></p></li>
</ul>
<p><span class="math notranslate nohighlight">\(MAE = \frac{\sum\limits_{i=1}^{n} |y_i - \hat{y}_i|}{n}\)</span></p>
<ul class="simple">
<li><p><strong>Median Absolute Error:</strong> again, an interpretable metric that is particularly interesting because it is robust to outliers, <span class="math notranslate nohighlight">\([0, +\infty)\)</span></p></li>
</ul>
<p><span class="math notranslate nohighlight">\(MedAE = median(|y_1 - \hat{y}_1|, ... , |y_n - \hat{y}_n|)\)</span></p>
<ul class="simple">
<li><p><strong>Mean Squared Error:</strong> the most commonly used metric that gives a higher penalty to large errors and vice versa, <span class="math notranslate nohighlight">\([0, +\infty)\)</span></p></li>
</ul>
<p><span class="math notranslate nohighlight">\(MSE = \frac{1}{n}\sum\limits_{i=1}^{n} (y_i - \hat{y}_i)^2\)</span></p>
<ul class="simple">
<li><p><strong>Mean Squared Logarithmic Error:</strong> practically, this is the same as MSE, but we take the logarithm of the series. As a result, we give more weight to small mistakes as well. This is usually used when the data has exponential trends, <span class="math notranslate nohighlight">\([0, +\infty)\)</span></p></li>
</ul>
<p><span class="math notranslate nohighlight">\(MSLE = \frac{1}{n}\sum\limits_{i=1}^{n} (log(1+y_i) - log(1+\hat{y}_i))^2\)</span></p>
<ul class="simple">
<li><p><strong>Mean Absolute Percentage Error:</strong> this is the same as MAE but is computed as a percentage, which is very convenient when you want to explain the quality of the model to management, <span class="math notranslate nohighlight">\([0, +\infty)\)</span></p></li>
</ul>
<p><span class="math notranslate nohighlight">\(MAPE = \frac{100}{n}\sum\limits_{i=1}^{n} \frac{|y_i - \hat{y}_i|}{y_i}\)</span></p>
</section>
<hr class="docutils" />
<section id="questions">
<h2>Questions<a class="headerlink" href="#questions" title="Permalink to this headline">#</a></h2>
<div class="tip dropdown admonition">
<p class="admonition-title">Problem: Cross Validation with Time Series</p>
<p>Can cross validation be used with Time Series to estimate model parameters automatically?</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Solution:</p>
<p>Normal cross-validation cannot be used for time series because one cannot randomly mix values in a fold while preserving this structure. With randomization, all time dependencies between observations will be lost. But something like “cross-validation on a rolling basis” can be used.</p>
<p>The idea is rather simple – we train our model on a small segment of the time series from the beginning until some <span class="math notranslate nohighlight">\(t\)</span>, make predictions for the next <span class="math notranslate nohighlight">\(t+n\)</span> steps, and calculate an error. Then, we expand our training sample to <span class="math notranslate nohighlight">\(t+n\)</span> value, make predictions from <span class="math notranslate nohighlight">\(t+n\)</span> until <span class="math notranslate nohighlight">\(t+2*n\)</span>, and continue moving our test segment of the time series until we hit the last available observation. As a result, we have as many folds as <span class="math notranslate nohighlight">\(n\)</span> will fit between the initial training sample and the last observation.</p>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Problem: CNNs in Time Series</p>
<p>How are CNNs used for Time Series Prediction?</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Solution:</p>
<ul class="simple">
<li><p>The ability of CNNs to learn and automatically extract features from raw input data can be applied to time series forecasting problems. A sequence of observations can be treated like a one-dimensional image that a CNN model can read and distill into the most salient elements.</p></li>
<li><p>The capability of CNNs has been demonstrated to great effect on time series classification tasks such as automatically detecting human activities based on raw accelerator sensor data from fitness devices and smartphones.</p></li>
<li><p>CNNs have the support for multivariate input, multivariate output, it can learn arbitrary but complex functional relationships, but does not require that the model learn directly from lag observations. Instead, the model can learn a representation from a large input sequence that is most relevant for the prediction problem.</p></li>
</ul>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Problem: Data Prep for Time Series</p>
<p>What are some of Data Preprocessing Operations you would use for Time Series Data?</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Solution:</p>
<p>It depends on the problem, but some common ones are:</p>
<ul class="simple">
<li><p>Parsing time series information from various sources and formats.</p></li>
<li><p>Generating sequences of fixed-frequency dates and time spans.</p></li>
<li><p>Manipulating and converting date times with time zone information.</p></li>
<li><p>Resampling or converting a time series to a particular frequency.</p></li>
<li><p>Performing date and time arithmetic with absolute or relative time increments.</p></li>
</ul>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Problem: Missing Value in Time Series</p>
<p>What are some of best ways to handle missing values in Time Series Data?</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Solution:</p>
<p>The most common methodology used for handling missing, unequally spaced, or unsynchronized values is <em>linear interpolation</em>.</p>
<p>The idea is to create estimated values at the desired time stamps. These can be used to generate multivariate time series that are synchronized, equally spaced, and have no missing values.
Consider the scenario where <span class="math notranslate nohighlight">\(y_i\)</span> and  <span class="math notranslate nohighlight">\(y_j\)</span> are values for the time series at times <span class="math notranslate nohighlight">\(t_i\)</span> and  <span class="math notranslate nohighlight">\(t_j\)</span>, repectively, where <span class="math notranslate nohighlight">\(i &lt; j\)</span>. Let <span class="math notranslate nohighlight">\(t\)</span> be a time drawn from the interval <span class="math notranslate nohighlight">\((t_i, t_j)\)</span>. Then, the interpolated value of the series is given by:
<span class="math notranslate nohighlight">\(y = y_i + \frac{t-t_i}{t_j-t_i}*(y_j-y_i)\)</span></p>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Problem: Stationarity</p>
<p>Can you explain why time series has to be stationary?</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Solution:</p>
<p>Stationarity is important because, in its absence, a model describing the data will vary in accuracy at different time points. As such, stationarity is required for sample statistics such as means, variances, and correlations to accurately describe the data at all time points of interest.</p>
<p>Looking at the time series plots below, you can notice how the mean and variance of any given segment of time would do a good job representing the whole stationary time series but a relatively poor job representing the whole non-stationary time series. For instance, the mean of the non-stationary time series is much lower from <span class="math notranslate nohighlight">\(600&lt;t&lt;800\)</span> and its variance is much higher in this range than in the range from <span class="math notranslate nohighlight">\(200&lt;t&lt;400\)</span>.</p>
<p><img alt="Stationary vs Non-Stationary" src="../../_images/image20.PNG" /></p>
<p>What quantities are we typically interested in when we perform statistical analysis on a time series? We want to know</p>
<ul class="simple">
<li><p>Its expected value,</p></li>
<li><p>Its variance, and</p></li>
<li><p>The correlation between values <span class="math notranslate nohighlight">\(s\)</span> periods apart for a set of values.</p></li>
</ul>
<p>To calculate these thingswe use a mean across many time periods. The mean across many time periods is only informative if the expected value is the same across those time periods. If these population parameters can vary, what are we really estimating by taking an average across time?</p>
<p>(Weak) stationarity requires that these population quantities must be the same across time, making the sample average a reasonable way to estimate them.</p>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Problem: IQR in Time Series</p>
<p>How is Interquartile range used in Time series?</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Solution:</p>
<p>It is mostly used to detect outliers in Time Series data.</p>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Problem: Irregular Data </p>
<p>What does irregularly-spaced spatial data mean in Time series?</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Solution:</p>
<ul class="simple">
<li><p>A lot of techniques assume that data is sampled at regularly-spaced intervals of time. This interval between adjacent samples is called the <em>sampling period</em>.</p></li>
<li><p>A lot of data is not or cannot be sampled with a fixed sampling period. For example, if we measure the atmosphere using sensors, the terrain may not allow us to place weather stations exactly 50 miles apart.</p></li>
</ul>
<p>There are many different ways to deal with this kind of data which does not have a fixed sampling period. One approach is to interpolate the data onto a grid and then use a technique intended for gridded data.</p>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Problem: Sliding Window </p>
<p>Explain the Sliding Window method in Time series?</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Solution:</p>
<ul class="simple">
<li><p>Time series can be phrased as supervised learning. Given a sequence of numbers for a time series dataset, we can restructure the data to look like a supervised learning problem.</p></li>
<li><p>In the sliding window method, the previous time steps can be used as input variables, and the next time steps can be used as the output variable.</p></li>
</ul>
<p>In statistics and time series analysis, this is called a lag or lag method. The number of previous time steps is called the window width or size of the lag. This sliding window is the basis for how we can turn any time series dataset into a supervised learning problem.</p>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Problem: LSTM vs MLP </p>
<p>Can you discuss on the usage of LSTM vs MLP in Time Series?</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Solution:</p>
<p>Multilayer Perceptrons, or MLPs for short, can be applied to time series forecasting. A challenge with using MLPs for time series forecasting is in the preparation of the data. Specifically, lag observations must be flattened into feature vectors. My understanding is that LSTMs captures the relations between time steps, whereas simple MLPs treat each time step as a separated feature (doesn’t take succession into consideration).</p>
<p>RNNs are known to be superior to MLP in case of sequential data. But complex models like LSTM and GRU require a lot of data to achieve their potential.</p>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Problem: Sequential vs Non-Sequential</p>
<p>Can a CNN (or other non-sequential deep learning models) outperform LSTM (or other sequential models) in time series data</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Solution:</p>
<p><em><a class="reference external" href="https://ai.stackexchange.com/questions/16818/can-non-sequential-deep-learning-models-outperform-sequential-models-in-time-ser">Source</a></em></p>
<p>You are right CNN based models can outperform RNN. You can take a look at this <a class="reference external" href="https://arxiv.org/pdf/1803.01271.pdf">paper</a> where they compared different RNN models with TCN (temporal convolutional networks) on different sequence modeling tasks. Even though there are no big differences in terms of results there are some nice properties that CNN based models offers such as: parallelism, stable gradients and low training memory footprint. In addition to CNN based models there are also attention based models (you might want to take a look at the <a class="reference external" href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">transformer</a>)</p>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Problem: Long Time Series</p>
<p>What’s the best architecture for time series prediction with a long dataset?</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Solution:</p>
<p><em><a class="reference external" href="https://ai.stackexchange.com/questions/17802/whats-the-best-architecture-for-time-series-prediction-with-a-long-dataset#:~:text=Yes%2C%20LSTM%20are%20ideal%20for%20this.">Source</a></em></p>
<p>LSTM is ideal for this. For even stronger representational capacity, make your LSTM’s multi-layered. Using 1-dimensional convolutions in a CNN is a common way to exctract information from time series too, so there’s no harm in trying. Typically, you’ll test many models out and take the one that has best validation performance.</p>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Problem: Correlation</p>
<p>How to use Correlation in Time series data?</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Solution:</p>
<p><em><a class="reference external" href="https://stats.stackexchange.com/questions/133155/how-to-use-pearson-correlation-correctly-with-time-series">Source</a></em></p>
<p>Pearson correlation is used to look at correlation between series … but being time series the correlation is looked at across different lags – the cross-correlation function. The cross-correlation is impacted by dependence within-series, so in many cases the within-series dependence should be removed first. So to use this correlation, rather than smoothing the series, it’s actually more common (because it’s meaningful) to look at dependence between residuals - the rough part that’s left over after a suitable model is found for the variables.</p>
<p>You probably want to begin with some basic resources on time series models before delving into trying to figure out whether a Pearson correlation across (presumably) nonstationary, smoothed series is interpretable.</p>
<p>In particular, you’ll probably want to look into spurious correlation. The point about spurious correlation is that series can appear correlated, but the correlation itself is not meaningful. Consider two people tossing two distinct coins counting number of heads so far minus number of tails so far as the value of their series.</p>
<p>(So if person-1 tosses <span class="math notranslate nohighlight">\(HTHH\)</span>… they have <span class="math notranslate nohighlight">\(3-1 = 2\)</span> for the value at the <span class="math notranslate nohighlight">\(4^{th}\)</span> time step, and their series goes <span class="math notranslate nohighlight">\(1,0,1,2,....\)</span>)</p>
<p>Obviously there’s no connection whatever between the two series. Clearly neither can tell you the first thing about the other!</p>
<p>But look at the sort of correlations you get between pairs of coins:</p>
<p><img alt="2 series for comparison" src="../../_images/image211.PNG" /></p>
<p>If I didn’t tell you what those were, and you took any pair of those series by themselves, those would be impressive correlations would they not?</p>
<p>But they’re all meaningless. Utterly spurious. None of the three pairs are really any more positively or negatively related to each other than any of the others – its just cumulated noise. The spuriousness isn’t just about prediction, the whole notion of of considering association between series without taking account of the within-series dependence is misplaced.</p>
<p>All you have here is within-series dependence. There’s no actual cross-series relation whatever.</p>
<p>Once you deal properly with the issue that makes these series auto-dependent - they’re all integrated (Bernoulli random walks), so you need to difference them - the “apparent” association disappears (the largest absolute cross-series correlation of the three is 0.048).</p>
<p>What that tells you is the truth – the apparent association is a mere illusion caused by the dependence within-series.</p>
<p>If your question asked “how to use Pearson correlation correctly with time series” – so please understand: if there’s within-series dependence and you don’t deal with it first, you won’t be using it correctly.</p>
<p>Further, smoothing won’t reduce the problem of serial dependence; quite the opposite – it makes it even worse! Here are the correlations after smoothing (default loess smooth - of series vs index - performed in R):</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>coin1</p></th>
<th class="head"><p>coin2</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>coin2</p></td>
<td><p>0.9696378</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>coin3</p></td>
<td><p>-0.8829326</p></td>
<td><p>-0.7733559</p></td>
</tr>
</tbody>
</table>
<p>They all got further from 0. They’re all still nothing but meaningless noise, though now it’s smoothed, cumulated noise. (By smoothing, we reduce the variability in the series we put into the correlation calculation, so that may be why the correlation goes up.)</p>
<p><em>Check the link given above for a detailed discussion</em></p>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Problem: State Space Model and Kalman Filtering</p>
<p>Describe in details how State Space Model and Kalman Filtering are used in Time Series forecasting?</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Solution:</p>
<p><em><a class="reference external" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwjDh_W1_dD6AhWU-DgGHY5OAlMQFnoECBMQAQ&amp;url=https%3A%2F%2Ftowardsdatascience.com%2Fstate-space-model-and-kalman-filter-for-time-series-prediction-basic-structural-dynamic-linear-2421d7b49fa6&amp;usg=AOvVaw0u-sCm-kITU5I-Ptdc9K8s">Resource</a> <a class="reference external" href="https://mfe.baruch.cuny.edu/wp-content/uploads/2014/12/TS_Lecture5_2019.pdf">Source</a></em></p>
<p>A state space model (SSM) is a time series model in which the time series <span class="math notranslate nohighlight">\(Y_t\)</span> is interpreted as the result of a noisy observation of a stochastic process <span class="math notranslate nohighlight">\(X_t\)</span>. The values of the variables <span class="math notranslate nohighlight">\(X_t\)</span> and <span class="math notranslate nohighlight">\(Y_t\)</span> can be continuous (scalar or vector) or discrete. Graphically, an SSM is represented as follows:</p>
<p><img alt="State Space Model" src="../../_images/image221.PNG" /></p>
<p>SSMs belong to the realm of Bayesian inference, and they have been successfully applied in many fields to solve a broad range of problems. It is usually assumed that the state process <span class="math notranslate nohighlight">\(X_t\)</span> is Markovian. The most well studied SSM is the Kalman filter, for which the processes above are linear and and the sources of randomness are Gaussian.</p>
<p>Let <span class="math notranslate nohighlight">\(T\)</span> denote the time horizon. Our broad goal is to make inference about the states Xt based on a set of observations <span class="math notranslate nohighlight">\(Y_1, . . . , Y_t\)</span>.
Three questions are of particular interest:</p>
<ul class="simple">
<li><p><strong>Filtering:</strong> <span class="math notranslate nohighlight">\(t &lt; T\)</span>. What can we infer about the current state of the system based on all available observations?</p></li>
<li><p><strong>Smoothing:</strong> <span class="math notranslate nohighlight">\(t = T\)</span>. What can be inferred about the system based on the information contained in the entire data sample? In particular, how can we back fill missing observations?</p></li>
<li><p><strong>Forecasting:</strong> <span class="math notranslate nohighlight">\(t &gt; T\)</span>. What is the optimal prediction of a future observation and/or a future state of the system?</p></li>
</ul>
<p>In principle, any inference for this model can be done using the standard methods of multivariate statistics. However, these methods require storing large amounts of data and inverting <span class="math notranslate nohighlight">\(tn × tn\)</span> matrices. Notice that, as new data arrive, the storage requirements and matrix dimensionality increase. This is frequently computationally intractable and impractical. Instead, the Kalman filter relies on a recursive approach which does not require significant storage resources and involves inverting <span class="math notranslate nohighlight">\(n × n\)</span> matrices only.</p>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Problem: State Space Model vs Conventional Methodologies</p>
<p>What are disadvantages of state-space models and Kalman Filter for time-series modelling over let’s say conventional methodologies like ARIMA, VAR or ad-hoc/heuristic methods?</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Solution:</p>
<p><em><a class="reference external" href="https://stats.stackexchange.com/questions/78287/what-are-disadvantages-of-state-space-models-and-kalman-filter-for-time-series-m">Source</a></em></p>
<ul class="simple">
<li><p>Overall - compared to ARIMA, state-space models allow you to model more complex processes, have interpretable structure and easily handle data irregularities; but for this you pay with increased complexity of a model, harder calibration, less community knowledge.</p></li>
<li><p>ARIMA is a universal approximator - you don’t care what is the true model behind your data and you use universal ARIMA diagnostic and fitting tools to approximate this model. It is like a polynomial curve fitting - you don’t care what is the true function, you always can approximate it with a polynomial of some degree.</p></li>
<li><p>State-space models naturally require you to write-down some reasonable model for your process (which is good - you use your prior knowledge of your process to improve estimates). Of course, if you don’t have any idea of your process, you always can use some universal state-space model also - e.g. represent ARIMA in a state-space form. But then ARIMA in its original form has more parsimonious formulation - without introducing unnecessary hidden states.</p></li>
<li><p>Because there is such a great variety of state-space models formulations (much richer than class of ARIMA models), behavior of all these potential models is not well studied and if the model you formulated is complicated - it’s hard to say how it will behave under different circumstances. Of course, if your state-space model is simple or composed of interpretable components, there is no such problem. But ARIMA is always the same well studied ARIMA so it should be easier to anticipate its behavior even if you use it to approximate some complex process.</p></li>
<li><p>Because state-space allows you directly and exactly model complex/nonlinear models, then for these complex/nonlinear models you may have problems with stability of filtering/prediction (EKF/UKF divergence, particle filter degradation). You may also have problems with calibrating complicated-model’s parameters - it’s a computationally-hard optimization problem. ARIMA is simple, has less parameters (1 noise source instead of 2 noise sources, no hidden variables) so its calibration is simpler.</p></li>
<li><p>For state-space there is less community knowledge and software in statistical community than for ARIMA.</p></li>
</ul>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Problem: Discrete Wavelet Transform</p>
<p>Have you heard of Discrete Wavelet Transform in time series?</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Solution:</p>
<p><em><a class="reference external" href="https://stackoverflow.com/questions/68303601/wavelet-for-time-series">Source</a></em></p>
<p>A Discrete Wavelet Transform (DWT) allows you to decompose your input data into a set of discrete levels, providing you with information about the frequency content of the signal i.e. determining whether the signal contains high frequency variations or low frequency trends. Think of it as applying several band-pass filters to your input data.</p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./contents\Algorithms"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="Tree%20based%20approaches.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Tree based approaches</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="Anomaly%20Detection.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Anomaly Detection</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Dip Ranjan Chatterjee | © MIT License - 2021<br/>
  
    <div class="extra_footer">
      <p>
<a href="https://dearc.medium.com/membership" target="_blank"><img src="https://img.shields.io/badge/Support-Medium%20Referral-lightgrey?style=flat-square&logo=appveyor.svg" alt="Medium Membership"></a>
<a href="https://www.linkedin.com/company/the-data-science-interview-book/?lipi=urn%3Ali%3Apage%3Ad_flagship3_feed%3BeglbXB3xT0mopZBzReqMEQ%3D%3D" target="_blank"><img src="https://img.shields.io/badge/Follow-LinkedIn-0077B5?style=flat-square&logo=appveyor.svg" alt="Follow LinkedIn"></a>
<a href="https://github.com/dipranjan/dsinterviewqns/discussions" target="_blank"><img src="https://img.shields.io/badge/Discussion%20Forum-%F0%9F%99%8A%20%20%F0%9F%99%88%20%20%F0%9F%99%89-0172B3?style=flat-square&logo=appveyor.svg" alt="Discussion Forum"></a>
<a href="https://www.buymeacoffee.com/dearc/e/88363" target="_blank"><img src="https://img.shields.io/badge/BUY-PDF%20Version%20%F0%9F%93%96-red?style=flat-square&logo=appveyor.svg" alt="Buy Book"></a>
</p>

    </div>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>