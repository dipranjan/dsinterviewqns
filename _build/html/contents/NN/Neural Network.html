
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />
<meta content="Interview resource of Data Science Interview focusing on Regression." lang="en" name="description" xml:lang="en" />
<meta content="interview, data science, machine learning, Neural Network, basics" name="keywords" />
<meta content="en_US" property="og:locale" />

    <title>Neural Network &#8212; The Data Science Interview Book</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://dipranjan.github.io/dsinterviewqns/contents/NN/Neural Network.html" />
    <link rel="shortcut icon" href="../../_static/logo.gif"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Recurrent Neural Network" href="Recurrent%20Neural%20Network.html" />
    <link rel="prev" title="Anomaly Detection" href="../Algorithms/Anomaly%20Detection.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-60403888-2', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.gif" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">The Data Science Interview Book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../intro.html">
   About this Book
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Journey of this project
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../To%20Do%20List.html">
   Log
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Statistics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Statistics/Probability%20Basics.html">
   Probability Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Statistics/Probability%20Distribution.html">
   Probability Distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Statistics/Central%20Limit%20Theorem.html">
   Central Limit Theorem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Statistics/Bayesian%20vs%20Frequentist%20Reasoning.html">
   Bayesian vs Frequentist Reasoning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Statistics/Hypothesis%20Testing.html">
   Hypothesis Testing
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Model Building
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Data/Data.html">
   Data
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Data/Scaling.html">
     Scaling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Data/Missing%20Value.html">
     Missing Value
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Data/Outlier.html">
     Outlier
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Data/Hyperparameter%20optimization.html">
   Hyperparameter Optimization
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Algorithms
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Algorithms/Regression.html">
   Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Algorithms/Generative%20VS%20Discriminative%20Models.html">
   Generative vs Discriminative Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Algorithms/Classification.html">
   Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Algorithms/Tree%20based%20approaches.html">
   Tree based approaches
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Algorithms/Time%20Series%20Analysis.html">
   Time Series Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Algorithms/Anomaly%20Detection.html">
   Anomaly Detection
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Neural Network
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Neural Network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Recurrent%20Neural%20Network.html">
   Recurrent Neural Network
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  NLP
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../NLP/Lexical%20Processing.html">
   Lexical Processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../NLP/Syntactic%20Processing.html">
   Syntactic Processing
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Python
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Python/Basics.html">
   Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Python/Data%20Manipulation.html">
   Data Manipulation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Python/Statistics.html">
   Statistics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Python/NLP.html">
   NLP
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  SQL
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../SQL/Select.html">
   SQL Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../SQL/Joins.html">
   Joins
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../SQL/Temporary%20Datasets.html">
   Temporary Datasets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../SQL/Windows%20Functions.html">
   Windows Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../SQL/Time.html">
   Time
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../SQL/Functions.html">
   Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../SQL/Problems.html">
   Problems
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Excel
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Excel/Excel%20Basics.html">
   Excel Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Excel/Data%20Manipulation.html">
   Data Manipulation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Excel/Time%20and%20Date.html">
   Time and Date
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tensorflow
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Tensorflow/Basics.html">
   Basics
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Analytical Thinking
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Analytical%20Thinking/Business%20Scenarios.html">
   Business Scenarios
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Analytical%20Thinking/Industry%20Application.html">
   Industry Application
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/contents/NN/Neural Network.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        
        <a class="issues-button"
            href="https://github.com/dipranjan/dsinterviewqns/issues/new?title=Issue%20on%20page%20%2Fcontents/NN/Neural Network.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#artificial-neuron">
   Artificial Neuron
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#feed-forward">
   Feed forward
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#backpropagation">
   Backpropagation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stochastic-gradient-descent-sgd">
     Stochastic Gradient Descent (SGD)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#modifications-in-neural-networks">
   Modifications in Neural Networks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dropouts">
     Dropouts
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Neural Network</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#artificial-neuron">
   Artificial Neuron
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#feed-forward">
   Feed forward
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#backpropagation">
   Backpropagation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stochastic-gradient-descent-sgd">
     Stochastic Gradient Descent (SGD)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#modifications-in-neural-networks">
   Modifications in Neural Networks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dropouts">
     Dropouts
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <section id="neural-network">
<h1>Neural Network<a class="headerlink" href="#neural-network" title="Permalink to this headline">Â¶</a></h1>
<div class="tip admonition">
<p class="admonition-title">ðŸ“– Resource</p>
<p><a class="reference external" href="http://neuralnetworksanddeeplearning.com/chap1.html">This is an excellent resource available for free on Neural Networks</a></p>
</div>
<p>Perceptrons were one of the earliest proposed models for learning simple classification tasks, which later became the fundamental building block of artificial neural networks.</p>
<section id="artificial-neuron">
<h2>Artificial Neuron<a class="headerlink" href="#artificial-neuron" title="Permalink to this headline">Â¶</a></h2>
<p>An artificial neuron is very similar to a perceptron, except that the activation function is not a step function.</p>
<p>Like a perceptron, the input is the weighted sum of inputs. The output is the activation function applied on the input. The activation function could be any function, though it should have some important properties such as:</p>
<ul class="simple">
<li><p>Activation functions should be smooth i.e. they should have no abrupt changes when plotted</p></li>
<li><p>They should also make the inputs and outputs non-linear with respect to each other to some extent. This is because non-linearity helps in making neural networks more compact</p></li>
</ul>
<p>Some of the common activation functions are as follows:</p>
<figure class="align-default" id="image1">
<a class="reference internal image-reference" href="../../_images/image19.PNG"><img alt="../../_images/image19.PNG" src="../../_images/image19.PNG" style="width: 588.4px; height: 546.8000000000001px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 19 </span><span class="caption-text"><a class="reference external" href="https://www.v7labs.com/blog/neural-networks-activation-functions">ðŸ“–Source</a></span><a class="headerlink" href="#image1" title="Permalink to this image">Â¶</a></p>
</figcaption>
</figure>
<p><strong>An artificial neural network is a network of such neurons.</strong> Neurons in a neural network are arranged in layers. The <em>first</em> and the <em>last</em> layer are called the <em>input</em> and <em>output</em> layers. Input layers have as many neurons as the number of attributes in the data set and the output layer has as many neurons as the number of classes of the target variable (for a classification problem). For a regression problem, the number of neurons in the output layer would be 1 (a numeric variable).
There are six main things that need to be specified for specifying a neural network completely:</p>
<ul class="simple">
<li><p>Network Topology</p></li>
<li><p>Input Layer</p></li>
<li><p>Output Layer</p></li>
<li><p>Weights</p></li>
<li><p>Activation functions</p></li>
<li><p>Biases</p></li>
</ul>
<p>An important thing to note is that the inputs can only be numeric. For different types of input data, we use different ways to convert the inputs to a numeric form. In case of text data, we either use a one-hot vector or word embeddings corresponding to a certain word. Feeding images (or videos) is straightforward since images are naturally represented as arrays of numbers.</p>
<p><strong>The output layer used in case of multiclass classification problem is the softmax layer.</strong> A softmax output is a multiclass logistic function commonly used to compute the â€˜probabilityâ€™ of an input belonging to one of the multiple classes.</p>
<p>Since large neural networks can potentially have extremely complex structures, certain assumptions are made to simplify the way information flows in them:</p>
<ul class="simple">
<li><p>Neurons are arranged in layers and the layers are arranged sequentially</p></li>
<li><p>Neurons within the same layer do not interact with each other</p></li>
<li><p>All the inputs enter the network through the input layer and all the outputs go out of the network through the output layer</p></li>
<li><p>Neurons in consecutive layers are densely connected, i.e. all neurons in layer <span class="math notranslate nohighlight">\(l\)</span> are connected to all neurons in layer <span class="math notranslate nohighlight">\(l+1\)</span></p></li>
<li><p>Every interconnection in the neural network has a weight associated with it, and every neuron has a bias associated with it</p></li>
<li><p>All neurons in a particular layer use the same activation function</p></li>
</ul>
<p>Neural networks require rigorous training. Recall that models such as linear regression, logistic regression, SVMs etc. are trained on their coefficients, i.e. the training task is to find the optimal values of the coefficients to minimize some cost function. Neural networks are no different - they are trained on weights and biases.</p>
<p>During training, the neural network learning algorithm fits various models to the training data and selects the best model for prediction. The learning algorithm is trained with a fixed set of hyperparameters - the network structure (number of layers, number of neurons in the input, hidden and output layers etc.). It is trained on the weights and the biases, which are the parameters of the network.</p>
<figure class="align-default" id="image2">
<a class="reference internal image-reference" href="../../_images/image2.gif"><img alt="../../_images/image2.gif" src="../../_images/image2.gif" style="width: 360.0px; height: 378.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 20 </span><span class="caption-text">Neural Networks can be used to <em>reasonably</em> approriximate <em>most</em> functions</span><a class="headerlink" href="#image2" title="Permalink to this image">Â¶</a></p>
</figcaption>
</figure>
</section>
<section id="feed-forward">
<h2>Feed forward<a class="headerlink" href="#feed-forward" title="Permalink to this headline">Â¶</a></h2>
<p>In artificial neural networks, the output from one layer is used as input to the next layer. Such networks are called feedforward neural networks.
Feed forward neural network architecture consists of following main parts â€“ Input Layer, Hidden Layer - If the number of hidden layer is one then it is known as a shallow neural network, else it is known as a deep neural network, Output Layer</p>
<figure class="align-default" id="image3">
<a class="reference internal image-reference" href="../../_images/image3.gif"><img alt="../../_images/image3.gif" src="../../_images/image3.gif" style="width: 448.0px; height: 336.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 21 </span><span class="caption-text"><a class="reference external" href="https://machinelearningknowledge.ai/animated-explanation-of-feed-forward-neural-network-architecture/">ðŸ“–Source</a></span><a class="headerlink" href="#image3" title="Permalink to this image">Â¶</a></p>
</figcaption>
</figure>
</section>
<section id="backpropagation">
<h2>Backpropagation<a class="headerlink" href="#backpropagation" title="Permalink to this headline">Â¶</a></h2>
<p>In the neural network training task the goal is to compute the optimal weights and biases by minimizing some cost function. The task of training neural networks is exactly the same as that of other ML models such as linear regression, SVMs etc. The desired output (output from the last layer) minus the actual output is the cost (or the loss), and we to tune the parameters w and b such that the total cost is minimized.</p>
<p>An important point to note is that if the data is large (which is often the case), loss calculation itself can get pretty messy. For example, if you have a million data points, they will be fed into the network (in batches), the output will be calculated using feedforward and the loss/cost <span class="math notranslate nohighlight">\(L_i\)</span> (for <span class="math notranslate nohighlight">\(i^{th}\)</span> data point) will be calculated. The total loss is the sum of losses of all the individual data points. We minimize the average of the total loss and the not the total loss. Minimizing the average loss implies that the total loss is getting minimized.</p>
<p>For a large neural network, the number of weight elements and biases becomes so large and minimizing the loss with so many parameters is a difficult task. This complex task is achieved using gradient descent.</p>
<figure class="align-default" id="image4">
<a class="reference internal image-reference" href="../../_images/image4.gif"><img alt="../../_images/image4.gif" src="../../_images/image4.gif" style="width: 448.0px; height: 336.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 22 </span><span class="caption-text"><a class="reference external" href="https://machinelearningknowledge.ai/animated-explanation-of-feed-forward-neural-network-architecture/">ðŸ“–Source</a></span><a class="headerlink" href="#image4" title="Permalink to this image">Â¶</a></p>
</figcaption>
</figure>
<p>For updating weights and biases using plain backpropagation, you have to scan through the entire data set to make a single update to the weights. This is computationally very expensive for large datasets. Thus, you use multiple batches (or <strong>mini-batches</strong>) of data points, compute the average gradient for a batch, and update the weights based on that gradient.</p>
<p>But there is a danger in doing this - you are making weight updates based only on gradients computed for small batches, not the entire training set. Thus, you make multiple passes through the entire training set using epochs. An epoch is one pass through the entire training set, and you use multiple epochs (typically 10, 20, 50, 100 etc.) while training. In each epoch, you reshuffle all the data points, divide the reshuffled set into m batches, and update weights based on gradient of each batch.</p>
<section id="stochastic-gradient-descent-sgd">
<h3>Stochastic Gradient Descent (SGD)<a class="headerlink" href="#stochastic-gradient-descent-sgd" title="Permalink to this headline">Â¶</a></h3>
<p>In most libraries such as Tensorflow, the SGD training procedure is as follows:</p>
<ul class="simple">
<li><p>You specify the number of epochs (typical values are 10, 20, 50, 100 etc.) - more epochs require more computational power</p></li>
<li><p>You specify the number of batches <span class="math notranslate nohighlight">\(m\)</span> (typical values are 32, 64, 128, etc.)</p></li>
<li><p>At the start of each epoch, the data set is reshuffled and divided into <span class="math notranslate nohighlight">\(m\)</span> batches</p></li>
<li><p>The average gradient of each batch is then used to make a weight update</p></li>
<li><p>The training is complete at the end of all the epochs</p></li>
</ul>
<p>Apart from being computationally faster, the SGD training process has another big advantage - it actually helps you reach the global minima (instead of being stuck at a local minima). Also, to avoid the problem of getting stuck at a local optimum, you need to strike a balance between exploration and exploitation.
Exploration means that you try to minimize the loss function with different starting points of <span class="math notranslate nohighlight">\(W\)</span> and <span class="math notranslate nohighlight">\(b\)</span>, i.e., you initialize <span class="math notranslate nohighlight">\(W\)</span> and <span class="math notranslate nohighlight">\(b\)</span> with different values. On the other hand, exploitation means that you try to reach the global minima starting from a particular W and b and do not explore the terrain at all. That might lead you to the lowest point locally, but not the necessarily the global minimum.</p>
</section>
</section>
<section id="modifications-in-neural-networks">
<h2>Modifications in Neural Networks<a class="headerlink" href="#modifications-in-neural-networks" title="Permalink to this headline">Â¶</a></h2>
<p>Neural networks are usually large, complex models with tens of thousands of parameters, and thus tend to overfit the training data. As with many other ML models, regularization is a common technique used in neural networks to address this problem.</p>
<p>The â€˜parameter normâ€™ regularization is similar to that in linear regression in almost every aspect. As in lasso regression (L1 norm), we get a sparse weight matrix, which is not the case with the L2 norm. Despite this fact, the L2 norm is more common because the sum of the squares term is easily differentiable which comes in handy during backpropagation.
Apart from using the parameter norm, there is another popular neural network regularization technique called <strong>dropouts</strong>.</p>
<section id="dropouts">
<h3>Dropouts<a class="headerlink" href="#dropouts" title="Permalink to this headline">Â¶</a></h3>
<p>Some important points to note regarding dropouts are:</p>
<ul class="simple">
<li><p>Dropouts can be applied only to some layers of the network (in fact, that is a common practice - you choose some layer arbitrarily to apply dropouts to)</p></li>
<li><p>The mask <span class="math notranslate nohighlight">\(\alpha\)</span> is generated independently for each layer during feedforward, and the same mask is used in backpropagation</p></li>
<li><p>The mask changes with each minibatch/iteration, are randomly generated in each iteration (sampled from a Bernoulli with some <span class="math notranslate nohighlight">\(p(1)=q\)</span>)</p></li>
</ul>
<p>Why the dropout strategy works well is explained through the notion of a manifold. Manifold captures the observation that in high dimensional spaces, the data points often actually lie in a lower-dimensional manifold. The dropout strategy uses this fact to find a lower-dimensional solution to the problem.</p>
<p>Dropouts help in symmetry breaking as well. There is every possibility of the creation of communities within neurons which restricts them from learning independently. Hence, by setting some random set of the weights to zero in every iteration, this community/symmetry is broken. Note that there, a different mini batch is processed in every iteration in an epoch, and dropouts are applied to each mini batch.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./contents\NN"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="../Algorithms/Anomaly%20Detection.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Anomaly Detection</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="Recurrent%20Neural%20Network.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Recurrent Neural Network</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Dip Ranjan Chatterjee | Â© MIT License - 2021<br/>
    
      <div class="extra_footer">
        <p>
<a href="https://www.buymeacoffee.com/dearc" target="_blank"><img src="https://img.shields.io/badge/Support-Buy%20me%20a%20%E2%98%95-orange?style=flat-square&logo=appveyor.svg" alt="Buy Me A Coffee"></a>
<a href="https://dearc.medium.com/membership" target="_blank"><img src="https://img.shields.io/badge/Support-Medium%20Referral-lightgrey?style=flat-square&logo=appveyor.svg" alt="Medium Membership"></a>
<a href="https://www.linkedin.com/company/the-data-science-interview-book/?lipi=urn%3Ali%3Apage%3Ad_flagship3_feed%3BeglbXB3xT0mopZBzReqMEQ%3D%3D" target="_blank"><img src="https://img.shields.io/badge/Follow-LinkedIn-0077B5?style=flat-square&logo=appveyor.svg" alt="Follow LinkedIn"></a>
<a href="https://github.com/dipranjan/dsinterviewqns/discussions" target="_blank"><img src="https://img.shields.io/badge/Discussion%20Forum-%F0%9F%99%8A%20%20%F0%9F%99%88%20%20%F0%9F%99%89-0172B3?style=flat-square&logo=appveyor.svg" alt="Discussion Forum"></a>
</p>

      </div>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>